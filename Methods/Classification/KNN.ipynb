{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3GlXFX15zjw",
        "colab_type": "text"
      },
      "source": [
        "## K Nearest Neighbors\n",
        "\n",
        "KNN is a classification ML algorithm that works with the following process:\n",
        "\n",
        "\n",
        "\n",
        "1.   Calculate the distance to every point\n",
        "2.   Choose the k closest points\n",
        "3.   Each point gets an equal vote for classifying the target point\n",
        "\n",
        "KNN can also be used for regression\n",
        "\n",
        "KNN is a simple example of a non-parametric model - a model which \"memorizes\" all of the training data and uses it at test time.\n",
        "\n",
        "As such there is no training phase and all of training data is used, meaning that the method is very computational expensive especially as the amount of data increases.\n",
        "\n",
        "Let's look at an implementation of KNN from scratch:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyWo1-g_9m2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "import math\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzYGCrUg9zSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mean(labels):\n",
        "  return np.sum(labels) / np.size(labels)\n",
        "\n",
        "def mode(labels):\n",
        "  return Counter(labels).most_common(1)[0][0]\n",
        "\n",
        "def eucl_dist(p1, p2):\n",
        "  axis_val = 0\n",
        "  if len(p1.shape) != 1:\n",
        "    axis_val = 1\n",
        "  return np.sqrt(np.sum(np.power(np.subtract(p1, p2), 2), axis = axis_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atS85KpcMtD9",
        "colab_type": "text"
      },
      "source": [
        "The above are helper methods to generalize the KNN algorithm that follows.\n",
        "\n",
        "As we see above, we've defined a distance function (Euclidean used here) and 2 choice functions:\n",
        "\n",
        "\n",
        "*   Mean will average the labels of the k nearest neighbors providing a regression answer\n",
        "*   Mode will yield the most common label of the k nearest neighbors, classifying the query data point\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICgS9J9h9r3I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def knn(data, query, k, distance_fn, choice_fn):\n",
        "  \n",
        "  data = np.asarray(data)\n",
        "\n",
        "  #strip labels, assumed to be last column of data\n",
        "  features = data[:,:-1]\n",
        "  labels = data[:,-1:].flatten()\n",
        "\n",
        "  distance = distance_fn(features, query)\n",
        "  \n",
        "  # if distance doesn't have the same length, distance_fn isn't properly vectorized\n",
        "  assert(len(distance) == len(data))\n",
        "\n",
        "  #sort features and labels by distance\n",
        "  inds = distance.argsort()\n",
        "  sort_feats = features[inds]\n",
        "  sort_labels = labels[inds]\n",
        "  \n",
        "  #return k closest indices, points and label\n",
        "  #choose label based on choice_fn, mode for classification, mean for regression\n",
        "  return dict(zip(inds[:k], sort_feats[:k])), choice_fn(sort_labels[:k])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}