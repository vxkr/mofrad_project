{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "PCA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLtsX8R90gqh",
        "colab_type": "text"
      },
      "source": [
        "## Principal component analysis\n",
        "\n",
        "#### Input: a matrix X, unlabeled data matrix\n",
        "\n",
        "#### Output: k directions of maximum variation\n",
        "\n",
        "PCA is a data feature reduction technique that can be especially useful in  \n",
        "machine learning applications. As a general data analysis tool, PCA yields  \n",
        "directions of maximum variance of the inputted data, and the corresponding  \n",
        "amount of variance captured by each of those directions. We choose some k, a  \n",
        "hyperparameter, which determines the number of vectors or 'directions' PCA  \n",
        "returns.\n",
        "\n",
        "As a dimensionality reduction tool, often when we work with data in machine  \n",
        "learning, the number of features (variables) can often be too numerous to work  \n",
        "with easily. The general issue is of increased computational complexity.  \n",
        "Instead, we can use PCA to choose the k most varied directions in the inputted  \n",
        "data, and then project the input data matrix onto those directions, resulting  \n",
        "in only k features.\n",
        "\n",
        "Note on hyperparameters: hyperparameters, such as k in PCA, are not values  \n",
        "determined by the algorithm to be the best for the problem. As such we must  \n",
        "think of ways to effectively validate our choice for k and compare how  \n",
        "different hyperparameter values effect our model. Here for example, we would k  \n",
        "to capture a large portion of the variance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JBNkkYF0gqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEwMK75j0gqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scratch_PCA(X, k):\n",
        "    demeaned_X= X - np.mean(X, axis=0)\n",
        "    #we now have to caculate the covariance matrix\n",
        "    cov_X= np.matmul(np.transpose(demeaned_X), demeaned_X)\n",
        "    sigma,V= np.linalg.eig(cov_X)\n",
        "    # you then select the k biggest eigenvalues and their corresponding eigenvectors\n",
        "    return sigma[:k], V[:k]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cgF_L3h2l8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def svd_PCA(X,k):\n",
        "  U, S, V = np.linalg.svd(X - np.mean(X, axis=0))\n",
        "  return S[:k]**2, V[:k]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GupVG9uS0gqp",
        "colab_type": "text"
      },
      "source": [
        "### Explanation\n",
        "\n",
        "In PCA we choose the first k components of the SVD of a matrix. In a nutshell,  \n",
        "SVD represents the breakdown of the input into a sum of rank 1 matrices. By  \n",
        "ordering the variance (the sigma values) by high to low, we capture the largest  \n",
        "amount of variance in the first k components. As we increase k to the rank of  \n",
        "the input, we fully capture the data matrix.\n",
        "\n",
        "#### Mathematically:\n",
        "\n",
        "SVD is represented as U * S * V.T\n",
        "\n",
        "* U - eigenvectors of X @ X.T\n",
        "* V - eigenvectors of X.T @ X\n",
        "* S - sigma squared = eigenvalues of both\n",
        "\n",
        "PCA uses the first k values of S and the first k vectors of V"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w77G72eF0gqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rQiOgzE0ssU",
        "colab_type": "text"
      },
      "source": [
        "In practice we would use modules such as sci-kit learn for running methods  \n",
        "such as PCA.\n",
        "\n",
        "Example extrapolated method below:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkSIHmn804dY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sk_PCA(X, k):\n",
        "  # initialize pca with num of components (k)\n",
        "  pca = PCA(n_components=k)\n",
        "\n",
        "  # run pca on our data set\n",
        "  pca.fit(X)\n",
        "\n",
        "  # get our first k components from our solutions\n",
        "  variance = pca.explained_variance_[:k]\n",
        "  vectors = pca.components_[:k]\n",
        "  vectors = [np.asarray(vectors[i]) for i in range(len(vectors))]\n",
        "\n",
        "  return variance, vectors"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}