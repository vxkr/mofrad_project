{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "PCA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLtsX8R90gqh",
        "colab_type": "text"
      },
      "source": [
        "## Principal component analysis\n",
        "\n",
        "\n",
        "Input: a matrix X, unlabeled data matrix\n",
        "\n",
        "\n",
        "Output: k directions of maximum variation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JBNkkYF0gqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEwMK75j0gqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scratch_PCA(X, k):\n",
        "    demeaned_X= X - np.mean(X, axis=0)\n",
        "    #we now have to caculate the covariance matrix\n",
        "    cov_X= np.matmul(np.transpose(demeaned_X), demeaned_X)\n",
        "    sigma,V= np.linalg.eig(cov_X)\n",
        "    # you then select the k biggest eignevalues and their corresponding eigenvectors\n",
        "    return sigma[:k], V[:k]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cgF_L3h2l8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def svd_PCA(X,k):\n",
        "  U, S, V = np.linalg.svd(X - np.mean(X, axis=0))\n",
        "  return S[:k]**2, V[:k]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GupVG9uS0gqp",
        "colab_type": "text"
      },
      "source": [
        "### Explanation\n",
        "\n",
        "In PCA we choose the first k components of the SVD of a matrix\n",
        "\n",
        "In a nutshell, SVD represents the breakdown of the input into a sum of rank 1 matrices.\n",
        "\n",
        "By ordering the variance (the sigma values) by high to low, we capture the largest amount of variance in the first k components.\n",
        "\n",
        "As we increase k to the rank of the input, we fully capture the data matrix.\n",
        "\n",
        "#### Mathematically:\n",
        "\n",
        "SVD is represented as U * S * V.T\n",
        "\n",
        "* U - eigenvectors of X @ X.T\n",
        "* V - eigenvectors of X.T @ X\n",
        "* S - sigma squared = eigenvalues of both\n",
        "\n",
        "PCA uses the first k values of S and the first k vectors of V"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w77G72eF0gqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rQiOgzE0ssU",
        "colab_type": "text"
      },
      "source": [
        "In practice we would use modules such as sci-kit learn for running methods such as PCA.\n",
        "\n",
        "Example extrapolated method below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkSIHmn804dY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sk_PCA(X, k):\n",
        "  # initialize pca with num of components (k)\n",
        "  pca = PCA(n_components=k)\n",
        "\n",
        "  # run pca on our data set\n",
        "  pca.fit(X)\n",
        "\n",
        "  # get our first k components from our solutions\n",
        "  variance = pca.explained_variance_[:k]\n",
        "  vectors = pca.components_[:k]\n",
        "  vectors = [np.asarray(vectors[i]) for i in range(len(vectors))]\n",
        "\n",
        "  return variance, vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gt7zd6afVTxi",
        "colab_type": "text"
      },
      "source": [
        "### Transformation\n",
        "\n",
        "So far the above methods help us get to the eigenvectors that represent the directions of maximum variation, and the sigma squared values representing the amount of variation. These vectors represent a basis we can project the original data onto, giving us the transformed data.\n",
        "\n",
        "This transformation would be carried our as followed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrBvskwgWNwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scratch_transform(X, k):\n",
        "  variance, pc = scratch_PCA(X, k)\n",
        "  return np.matmul(X, np.transpose(np.asarray(pc)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilWA3KwCW5_B",
        "colab_type": "text"
      },
      "source": [
        "And instead using the methods from scikit-learn:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H24TOgUAW5ES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sk_transform(X,k):\n",
        "  pca = PCA(n_components=k)\n",
        "  return pca.fit_transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}